
In which scenarios we need to increase driver cores ?

some explanation on how stages are created when we try to apply some transformations/actions ?

Can you explain how many partition is created by default in read file ?
Could you please elaborate, how things are handled if a certain task gets failed?
assignment of partition to a task by driver takes place randomly or by any logic?
When we run spark job on local in REPL, then is this happening, 0 worker nodes with 1 driver/master node?
how are the number of tasks calculated within spark - app?
How many spark-shell we can open at once ?
can we perform agg transform like finding max value from a RDD?

Can u please show how to create multiple spark sessions after this ?


memory configuration is not working for me, getting below error:
Invalid maximum heap size: -Xmx6G
The specified size exceeds the maximum representable size.
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

